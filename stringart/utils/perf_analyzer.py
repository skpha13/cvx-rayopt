import json
import logging
import os
import string
import time
import tracemalloc
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, List

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import gridspec
from matplotlib.colors import Colormap
from skimage import io
from stringart.solver import Solver
from stringart.utils.image import ImageWrapper, crop_image, masked_rmse
from stringart.utils.time_memory_format import (
    convert_memory_size,
    convert_monotonic_time,
    format_memory_size,
    format_time,
)
from stringart.utils.types import CropMode, ElapsedTime, MemorySize, Rasterization
from tqdm import tqdm

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """A dataclass that stores the results of a benchmark.

    Attributes
    ----------
    solver: str
        The name of the solver ran by the benchmark.
    params: dict
        The params passed to the solver.
    number_of_pegs: int, optional
        The number of pegs to be used in the string art computation. Default is 128.
    shape : tuple of int
        The shape of the image (height, width) used in the benchmark.
    crop_mode : CropMode
        The crop mode used in the image preprocessing.
    rasterization : Rasterization
        The rasterization strategy used in generating the string art.
    block_size : int | None
        Size of the square block used in downsampling via averaging.
    output_image : np.ndarray | None
        The image output generated by the benchmarked function.
    residual_history : list[np.floating]
        The history of residual values (errors) during the solver iterations.
    x : np.ndarray
        The solution vector or array computed by the solver.
    elapsed_monotonic_time : float
        The elapsed time in seconds measured with `time.monotonic()`.
    peak_memory_usage : int
        The peak memory usage in bytes measured by `tracemalloc.get_traced_memory()`.
    elapsed_time : ElapsedTime
        The elapsed time as a dictionary containing hours, minutes, seconds, and milliseconds.
    peak_memory_size : MemorySize
        The peak memory usage as a dictionary containing gigabytes, megabytes, kilobytes, and bytes.
    """

    solver: str
    params: dict
    number_of_pegs: int
    shape: tuple[int, ...]
    crop_mode: CropMode
    rasterization: Rasterization
    output_image: np.ndarray
    output_image_path: str | None
    block_size: int | None

    residual_history: list[np.floating]
    x: np.ndarray

    elapsed_monotonic_time: float
    peak_memory_usage: int
    elapsed_time: ElapsedTime
    peak_memory_size: MemorySize

    def __str__(self):
        formatted_time = format_time(self.elapsed_time)
        formatted_memory = format_memory_size(self.peak_memory_size)

        return (
            f"Benchmark Results:\n"
            f"- Solver: {self.solver}\n"
            f"- Params: {json.dumps(self.params, indent=4)}\n"
            f"- Number of Pegs: {self.number_of_pegs}\n"
            f"- Shape: {self.shape}\n"
            f"- Crop Mode: {self.crop_mode}\n"
            f"- Rasterization: {self.rasterization}\n"
            f"- Block Size: {self.block_size}\n"
            f"- Elapsed Time: {formatted_time}\n"
            f"- Peak Memory Usage: {formatted_memory}\n"
            f"- Output Image Path: {self.output_image_path}\n"
            f"- Best Residual: {self.residual_history[-1]:.6f}"
        )

    def to_json(self) -> dict:
        return {
            "solver": self.solver,
            "params": self.params,
            "number_of_pegs": self.number_of_pegs,
            "shape": self.shape,
            "crop_mode": self.crop_mode,
            "rasterization": self.rasterization,
            "block_size": self.block_size,
            "output_image_path": self.output_image_path,
            "elapsed_monotonic_time": self.elapsed_monotonic_time,
            "peak_memory_usage": self.peak_memory_usage,
            "elapsed_time": self.elapsed_time,
            "peak_memory_size": self.peak_memory_size,
            "residual_history": self.residual_history,
            "x": self.x.tolist(),
        }


class Benchmark:
    PLOTS_PATH: Path | None = None
    BENCHMARKS_PATH: Path | None = None
    BENCHMARKS_IMAGE_OUTPUT_PATH: Path | None = None

    @staticmethod
    def initialize_metadata(path: Path) -> None:
        Benchmark.PLOTS_PATH = path / "docs/plots"
        Benchmark.BENCHMARKS_PATH = path / "benchmarks"
        Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH = path / "benchmarks/img_outputs"

        os.makedirs(Benchmark.PLOTS_PATH, exist_ok=True)
        os.makedirs(Benchmark.BENCHMARKS_PATH, exist_ok=True)
        os.makedirs(Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH, exist_ok=True)

    def __init__(
        self,
        image: np.ndarray,
        crop_mode: CropMode | None,
        number_of_pegs: int | None = 128,
        rasterization: Rasterization | None = "xiaolin-wu",
        block_size: int | None = 2,
    ):
        """A class to perform benchmarking on various stringart solving methods.

        Parameters
        ----------
        image : np.ndarray
            The input image to be processed.
        crop_mode : CropMode, optional. Defaults to 'center'.
            The mode in which the image is being processed. This determines cropping behaviour.
        number_of_pegs : int, optional. Defaults to 128.
            The number of pegs used in the solving process. Default is 128.
        rasterization: Rasterization, optional. Defaults to 'xiaolin-wu'
            If "xiaolin-wu", the line is generated using a rasterized algorithm (Xiaolin Wu's algorithm).
            If "bresenham", the line is generated using a non-rasterized algorithm (Bresenham's algorithm).
        block_size : int | None. Defaults to 2.
            Size of the square block used in downsampling via averaging.

        Attributes
        ----------
        image : np.ndarray
            The input image to be processed.
        crop_mode : CropMode
            The mode in which the image is being processed. This determines cropping behaviour.
        number_of_pegs : int
            The number of pegs used in the solving process.
        rasterization: Rasterization
            If "xiaolin-wu", the line is generated using a rasterized algorithm (Xiaolin Wu's algorithm).
            If "bresenham", the line is generated using a non-rasterized algorithm (Bresenham's algorithm).
        solver : Solver
            The solver instance configured with the provided parameters.
        benchmarks_to_run : list
            A list of tuples defining the solver methods to benchmark and their respective parameters.
        """
        crop_mode: CropMode = crop_mode if crop_mode else "center"
        number_of_pegs = number_of_pegs if number_of_pegs else 128
        rasterization: Rasterization = rasterization if rasterization else "xiaolin-wu"
        block_size: int = block_size if block_size else 2

        self.image = image
        self.crop_mode = crop_mode
        self.number_of_pegs = number_of_pegs
        self.rasterization = rasterization
        self.block_size = block_size

        self.solver = Solver(
            image,
            crop_mode,
            number_of_pegs=number_of_pegs,
            rasterization=rasterization,
            block_size=block_size,
        )
        self.benchmarks_to_run = [
            # fmt: off
            (self.solver.ls, {"matrix_representation": "dense"}, False),
            (self.solver.ls, {"matrix_representation": "sparse"}, False),
            (self.solver.ls, {"matrix_representation": "sparse", "number_of_lines": 1000}, False),
            (self.solver.ls, {"matrix_representation": "sparse", "number_of_lines": 1000, "binary": True}, True),
            (self.solver.lls, {"matrix_representation": "sparse", "number_of_lines": 1000}, False),
            (self.solver.lls, {"matrix_representation": "sparse", "number_of_lines": 1000, "binary": True}, True),
            (self.solver.lsr, {"regularizer": None, "lambd": 100}, False),
            (self.solver.lsr, {"regularizer": "smooth", "lambd": 100}, False),
            (self.solver.mp, {"number_of_lines": 1000, "mp_method": "greedy", "selector_type": "all"}, True),
            (self.solver.mp, {"number_of_lines": 1000, "mp_method": "greedy", "selector_type": "dot-product"}, True),
            (self.solver.mp, {"number_of_lines": 1000, "mp_method": "greedy", "selector_type": "random"}, True),
            (self.solver.mp, {"number_of_lines": 1000, "mp_method": "orthogonal"}, True),
            (self.solver.bpls, {"solver": "scipy", "k": 3, "max_iterations": 1000}, True),
            (self.solver.bpls, {"solver": "cvxopt", "k": 3, "max_iterations": 1000}, True),
            (self.solver.radon, {}, True),
            # fmt: on
        ]

    def run_benchmark(self, func: Callable, uds: bool = False, *args, **kwargs) -> BenchmarkResult:
        """Benchmark a function by measuring its execution time and peak memory usage.

        Parameters
        ----------
        func : Callable
           The function to be benchmarked.
        uds: bool
            If set, it will run the supersampling compute solution. Defaults to `False`.
        *args : tuple
           Positional arguments passed to the function.
        **kwargs : dict
           Keyword arguments passed to the function.

        Returns
        -------
        BenchmarkResult
           A BenchmarkResult instance containing the output of the function,
           elapsed time, and peak memory usage.
        """

        time_start = time.monotonic()
        tracemalloc.start()

        A, x, residuals = func(*args, **kwargs)
        output = self.solver.compute_solution(A, x, uds=uds)

        time_end = time.monotonic()
        elapsed_monotonic_time = time_end - time_start

        _, peak_memory_usage = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        elapsed_time: ElapsedTime = convert_monotonic_time(elapsed_monotonic_time)
        peak_memory_size: MemorySize = convert_memory_size(peak_memory_usage)

        return BenchmarkResult(
            solver=func.__name__,
            params=kwargs,
            number_of_pegs=self.number_of_pegs,
            shape=output.shape,
            crop_mode=self.crop_mode,
            rasterization=self.rasterization,
            block_size=self.block_size,
            output_image=output,
            output_image_path=None,
            elapsed_monotonic_time=elapsed_monotonic_time,
            peak_memory_usage=peak_memory_usage,
            elapsed_time=elapsed_time,
            peak_memory_size=peak_memory_size,
            residual_history=residuals,
            x=x,
        )

    def run_benchmarks(self) -> List[BenchmarkResult]:
        """Run a series of benchmark tests on a given image using different solvers and configurations.

        Returns
        -------
        List[BenchmarkResult]
            A list of benchmark results, each containing the solver, parameters, and performance
            metrics (e.g., elapsed time, memory usage) for each benchmark run.
        """

        results: List[BenchmarkResult] = []

        for benchmark_to_run in tqdm(self.benchmarks_to_run, desc="Running Benchmarks"):
            func, params, uds = benchmark_to_run

            logger.info(f"===== Starting Benchmark =====")
            logger.info(f"Function: {func.__name__}")
            logger.info(f"Parameters: {params}")

            try:
                result = self.run_benchmark(func, uds, **params)
                results.append(result)
            except Exception as e:
                logger.exception(f"Benchmark failed for {func.__name__} with params {params}. Exception: {e}")
                continue

            logger.info(f"Completed Function: {func.__name__}")

        return results

    @staticmethod
    def save_benchmarks(benchmark_results: List[BenchmarkResult], filename: str = "benchmark") -> None:
        """Save benchmark results to a JSON file.

        Parameters
        ----------
            benchmark_results: List[BenchmarkResult]
                List of benchmark results to save.
            filename: str
                Name of the output JSON file.
        """

        directory = Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH / filename
        os.makedirs(directory, exist_ok=True)

        # save each output_image to a directory inside benchmarks named after filename
        for index in range(len(benchmark_results)):
            output_image = benchmark_results[index].output_image

            image_name = f"image_{index:03}.png"
            io.imsave(directory / image_name, output_image)

            benchmark_results[index].output_image_path = str(directory / image_name)

        json_results = [result.to_json() for result in benchmark_results]
        filepath = Benchmark.BENCHMARKS_PATH / f"{filename}.json"
        with open(filepath, "w") as file:
            json.dump(json_results, file, indent=4)

        logger.info(f"Benchmarks saved to: {filepath}")

    @staticmethod
    def load_benchmarks(filename: str) -> List[BenchmarkResult]:
        """Load benchmark data from a JSON file, process the data, and return a list of
        BenchmarkResult objects.

        Parameters
        ----------
        filename : str
            The name of the JSON file (without the .json extension) that contains the benchmark data.
            The benchmark data should be found in the `benchmarks` directory.

        Returns
        -------
        List[BenchmarkResult]
            A list of `BenchmarkResult` objects created from the benchmark data in the JSON file.
        """
        filepath = Benchmark.BENCHMARKS_PATH / f"{filename}.json"
        with open(filepath, "r") as file:
            benchmarks = json.load(file)

        for benchmark in benchmarks:
            benchmark["output_image"] = io.imread(benchmark["output_image_path"])

        benchmarks_class = [
            BenchmarkResult(**benchmark) for benchmark in benchmarks  # unpack dict values into constructor
        ]

        logger.info(f"Loaded benchmarks: {filepath}")
        return benchmarks_class

    def run_analysis(
        self, benchmarks: List[BenchmarkResult], ground_truth_image: np.ndarray, dirname: str = "analysis"
    ) -> None:
        """Perform an analysis of benchmarking results by generating and saving plots that show the differences
        between output images and the ground truth image, as well as RMSE and time and memory usage for each benchmark.
        Additionally, it also computes the residual history and normalized residual history over iterations.

        Parameters
        ----------
        benchmarks : List[BenchmarkResult]
            A list of benchmark results to analyze.
        ground_truth_image : np.ndarray
            The ground truth image to compare the benchmarked images against. It is expected to be a
            2D array representing the image in a NumPy array format.
        dirname : str, optional. Defaults to "analysis".
            The directory where the analysis results (plots) will be saved. If the directory does not
            exist, it will be created inside the `docs/plots` directory.
        """

        directory = self.PLOTS_PATH / dirname
        os.makedirs(directory, exist_ok=True)

        # create subdirectory for diff images
        diff_dir = directory / "diff_images"
        os.makedirs(diff_dir, exist_ok=True)

        # images are scaled in [0, 1] range because the solvers return them in range [0, 255]
        output_images = [ImageWrapper.scale_image(benchmark.output_image) for benchmark in benchmarks]
        ground_truth_image = crop_image(ground_truth_image, self.crop_mode)
        ground_truth_image_alpha = ImageWrapper.apply_alpha_map_bw_to_rgba(
            ground_truth_image, ImageWrapper.alpha_map(ground_truth_image, self.crop_mode)
        )

        base_labels = [benchmark.solver for benchmark in benchmarks]
        letter_labels = list(string.ascii_lowercase[: len(base_labels)])
        combined_labels = [f"({letter}) {solver}" for letter, solver in zip(letter_labels, base_labels)]

        # plot diff images and rmses
        rmses = [
            masked_rmse(
                ground_truth_image_alpha,
                ImageWrapper.apply_alpha_map_bw_to_rgba(test_image, ImageWrapper.alpha_map(test_image, self.crop_mode)),
            )
            for test_image in output_images
        ]
        diff_images = [
            ImageWrapper.scale_image(np.abs(ground_truth_image - test_image)) for test_image in output_images
        ]
        diff_images = prepare_diff_images(diff_images, self.crop_mode)

        # save diff images with heatmap
        for idx, diff_image in enumerate(diff_images):
            filename = f"diff_image_{idx:03}.png"
            plt.figure(figsize=(8, 8))
            plt.imshow(diff_image, cmap="plasma")
            plt.axis("off")
            plt.tight_layout()
            plt.savefig(diff_dir / filename, format="png", bbox_inches="tight", pad_inches=0)
            plt.close()

        plot_name = "Difference Images"
        max_cols = 5

        num_images = len(output_images)
        num_rows = (num_images + max_cols) // max_cols

        image_width = 3
        image_height = 3
        text_height = 0.5

        fig_width = max_cols * image_width
        fig_height = num_rows * (2 * image_height + text_height)

        fig = plt.figure(figsize=(fig_width, fig_height))
        gs = gridspec.GridSpec(
            num_rows * 3,
            max_cols,
            figure=fig,
            height_ratios=[image_height, image_height, text_height] * num_rows,  # 1:1 for images, 0.5 for text
            hspace=0,
            wspace=0.05,
        )
        axs = np.empty((num_rows * 3, max_cols), dtype=object)

        for row in range(num_rows * 3):
            for col in range(max_cols):
                axs[row, col] = fig.add_subplot(gs[row, col])
                axs[row, col].set_axis_off()

        # first image: ground truth
        axs[0, 0].imshow(ground_truth_image_alpha, cmap="gray")
        axs[0, 0].set_axis_off()

        # create a gradient image for colorbar display (1x256 gradient)
        ax_colorbar = axs[1, 0]
        ax_colorbar.set_axis_off()
        pos = ax_colorbar.get_position()
        cbar_width = 0.02
        cbar_x = pos.x0 + (pos.width - cbar_width) / 2
        cbar_height = pos.height * 0.8
        cbar_y = pos.y0 + (pos.height - cbar_height) / 2
        cbar_ax = fig.add_axes([cbar_x, cbar_y, cbar_width, cbar_height])
        norm = matplotlib.colors.Normalize(vmin=0, vmax=1)
        cbar = matplotlib.colorbar.ColorbarBase(cbar_ax, cmap=matplotlib.cm.plasma, norm=norm, orientation="vertical")
        cbar_ax.tick_params(labelsize=8)

        # text label for ground truth
        axs[2, 0].text(
            0.5,
            0.5,
            f"({letter_labels[0]})\nTarget Image",
            ha="center",
            va="center",
            transform=axs[2, 0].transAxes,
            fontsize=12,
            wrap=True,
        )

        # plot remaining output images and diff images starting from index 1
        for idx in range(1, num_images + 1):
            col = idx % max_cols
            row_group = (idx // max_cols) * 3

            # row 1: output image
            axs[row_group, col].imshow(output_images[idx - 1], cmap="gray")
            axs[row_group, col].set_axis_off()

            # row 2: diff image + label
            axs[row_group + 1, col].imshow(diff_images[idx - 1], cmap="plasma")
            axs[row_group + 1, col].set_axis_off()

            # row 3: text label with RMS
            axs[row_group + 2, col].text(
                0.5,
                0.5,
                combined_labels[idx - 1] + f"\nRMS: {rmses[idx - 1]:.4f}",
                ha="center",
                va="center",
                transform=axs[row_group + 2, col].transAxes,
                fontsize=12,
                wrap=True,
            )

        # hide any unused axes
        total_slots = num_rows * max_cols
        for empty_idx in range(num_images, total_slots):
            col = empty_idx % max_cols
            row_group = (empty_idx // max_cols) * 3
            for r in range(3):
                axs[row_group + r, col].axis("off")

        fig.subplots_adjust(wspace=0, hspace=0.01)
        fig.savefig(f"{directory / f'{plot_name}.png'}", format="png", bbox_inches="tight", pad_inches=0)
        plt.close(fig)

        # plot residual over iterations
        raw_residuals = [np.array(benchmark.residual_history) for benchmark in benchmarks]
        plot_residuals(raw_residuals, combined_labels, "Residual History", directory)

        # flatten all residuals to find global min and max
        all_residuals = [r for benchmark in benchmarks for r in benchmark.residual_history]
        global_min = np.min(all_residuals)
        global_max = np.max(all_residuals)
        range_eps = global_max - global_min if global_max != global_min else 1e-8  # avoid division by zero

        normalized_residuals = []
        for residuals in raw_residuals:
            if len(residuals) == 1:
                # global normalization
                normalized = (residuals - global_min) / range_eps
            else:
                # local normalization
                normalized = (residuals - np.min(residuals)) / (np.max(residuals) - np.min(residuals) + 1e-8)

            normalized_residuals.append(normalized)

        plot_residuals(normalized_residuals, combined_labels, "Normalized Residual History", directory)

        # plot time and memory
        monotonic_time = [benchmark.elapsed_monotonic_time for benchmark in benchmarks]
        memory_size = [benchmark.peak_memory_usage / (1024**2) for benchmark in benchmarks]

        def plot_bar_graph(
            name: str, ylabel: str, values: List | np.ndarray, color: str = "skyblue", log_scale: bool = False
        ) -> None:
            plt.figure(figsize=(10, 6))

            bar_width = 0.6
            plt.bar(combined_labels, values, color=color, width=bar_width)

            plt.xlabel("Solvers")
            plt.ylabel(ylabel)

            if log_scale:
                plt.yscale("log")

            plt.xticks(rotation=45)
            plt.tight_layout()

            plt.savefig(f"{directory / name}.png", format="png")
            plt.show()

        # time plot
        plot_bar_graph("Time Usage", "Time (s)", monotonic_time, "skyblue", log_scale=True)
        # memory plot
        plot_bar_graph("Memory Usage", "Memory (MB)", memory_size, "orange")

        logger.info(f"Analysis saved to: {directory}")


def prepare_diff_images(
    diff_images: list[np.ndarray], crop_mode: CropMode, colormap: str | Colormap = "plasma"
) -> list[np.ndarray]:
    """Processes a list of difference images by applying an alpha map and colormap to each image, converting them to RGBA.

    Parameters
    ----------
    diff_images : list of np.ndarray
        A list of 2D NumPy arrays (height, width) representing difference images. Each image should be in black-and-white (grayscale).
    crop_mode : CropMode
        The cropping mode that was used on the diff_images.
    colormap : str or matplotlib.colors.Colormap, optional. Defaults to 'plasma'.
        The name of the Matplotlib colormap to apply or a Colormap object.

    Returns
    -------
    list of np.ndarray
        A list of 3D NumPy arrays (height, width, 4) representing the processed RGBA images. Each image will have the
        red, green, and blue channels updated with a colormap (plasma), and the alpha channel applied from the corresponding
        alpha map based on the mode.
    """
    cmap = plt.get_cmap(colormap) if isinstance(colormap, str) else colormap

    def apply_cmap_bw_to_rgb(rgba_image: np.ndarray) -> np.ndarray:
        bw_image = rgba_image[..., 0]

        colorized = cmap(bw_image)
        rgba_image[..., 0:3] = colorized[..., 0:3]

        return rgba_image

    diff_images = [
        apply_cmap_bw_to_rgb(
            ImageWrapper.apply_alpha_map_bw_to_rgba(diff_image, ImageWrapper.alpha_map(diff_image, crop_mode))
        )
        for diff_image in diff_images
    ]

    return diff_images


def plot_residuals(y_data: List[np.ndarray], labels: List[str], plot_name: str, directory: str | Path) -> None:
    """Plot residuals over iterations for multiple benchmarks and save the resulting figure.

    Each series in `y_data` corresponds to a sequence of residuals for a specific benchmark.
    If a residual sequence contains only one value, it will be plotted as a single point.

    Parameters
    ----------
    y_data : List[np.array]
        A list of 1D NumPy arrays where each array represents residuals for a benchmark over iterations.
    labels : List[str]
        A list of strings used as labels for each benchmark's residual plot. Must be the same length as `y_data`.
    plot_name : str
        Title of the plot and the filename used when saving the image (without file extension).
    directory : str or Path
        Directory where the resulting plot image will be saved. If it does not exist, it should be created beforehand.
    """

    fig, ax = plt.subplots(figsize=(12, 6))

    for idx, (y, label) in enumerate(zip(y_data, labels)):
        if len(y) == 1:
            ax.plot(2 * idx, y[0], "o", label=label)
        else:
            ax.plot(range(len(y)), y, label=label)

    ax.set_xlabel("Iteration")
    ax.set_ylabel("Residual")
    ax.legend(fontsize=8, loc="upper left", bbox_to_anchor=(1.05, 1), borderaxespad=0.0)
    ax.grid(True)

    fig.tight_layout()
    fig.savefig(f"{directory / plot_name}.png", format="png")
    fig.show()
