import json
import logging
import os
import time
import tracemalloc
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, List

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.colors import Colormap
from skimage import io
from skimage.metrics import normalized_root_mse
from stringart.solver import Solver
from stringart.utils.image import ImageWrapper, crop_image
from stringart.utils.time_memory_format import (
    convert_memory_size,
    convert_monotonic_time,
    format_memory_size,
    format_time,
)
from stringart.utils.types import CropMode, ElapsedTime, MemorySize, Rasterization
from tqdm import tqdm

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """A dataclass that stores the results of a benchmark, including the output image,
    elapsed time, and peak memory usage.

    Attributes
    ----------
    solver: str
        The name of the solver ran by the benchmark.
    params: dict
        The params passed to the solver.
    number_of_pegs: int, optional
        The number of pegs to be used in the string art computation. Default is 100.
    output_image : np.ndarray
        The image output generated by the benchmarked function.
    elapsed_monotonic_time : float
        The elapsed time in seconds measured with `time.monotonic()`.
    peak_memory_usage : int
        The peak memory usage in bytes measured by `tracemalloc.get_traced_memory()`.
    elapsed_time : ElapsedTime
        The elapsed time as a dictionary containing hours, minutes, seconds, and milliseconds.
    peak_memory_size : MemorySize
        The peak memory usage as a dictionary containing gigabytes, megabytes, kilobytes, and bytes.
    """

    solver: str
    params: dict
    number_of_pegs: int
    output_image: np.ndarray
    output_image_path: str | None
    elapsed_monotonic_time: float
    peak_memory_usage: int

    elapsed_time: ElapsedTime
    peak_memory_size: MemorySize

    def __str__(self):
        formatted_time = format_time(self.elapsed_time)
        formatted_memory = format_memory_size(self.peak_memory_size)

        return (
            f"Benchmark Results:\n"
            f"- Solver: {self.solver}\n"
            f"- Params: {json.dumps(self.params, indent=4)}\n"
            f"- Number of Pegs: {self.number_of_pegs}\n"
            f"- Elapsed Time: {formatted_time}\n"
            f"- Peak Memory Usage: {formatted_memory}\n"
            f"- Output Image Shape: {self.output_image.shape}\n"
            f"- Output Image Path: {self.output_image_path}"
        )

    def to_json(self) -> dict:
        return {
            "solver": self.solver,
            "params": self.params,
            "number_of_pegs": self.number_of_pegs,
            "output_image_shape": self.output_image.shape,
            "output_image_path": self.output_image_path,
            "elapsed_monotonic_time": self.elapsed_monotonic_time,
            "peak_memory_usage": self.peak_memory_usage,
            "elapsed_time": self.elapsed_time,
            "peak_memory_size": self.peak_memory_size,
        }


class Benchmark:
    PLOTS_PATH: Path | None = None
    BENCHMARKS_PATH: Path | None = None
    BENCHMARKS_IMAGE_OUTPUT_PATH: Path | None = None

    @staticmethod
    def initialize_metadata(path: Path) -> None:
        Benchmark.PLOTS_PATH = path / "docs/plots"
        Benchmark.BENCHMARKS_PATH = path / "benchmarks"
        Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH = path / "benchmarks/img_outputs"

        os.makedirs(Benchmark.PLOTS_PATH, exist_ok=True)
        os.makedirs(Benchmark.BENCHMARKS_PATH, exist_ok=True)
        os.makedirs(Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH, exist_ok=True)

    def __init__(
        self,
        image: np.ndarray,
        crop_mode: CropMode | None,
        number_of_pegs: int | None = 100,
        rasterization: Rasterization | None = "bresenham",
    ):
        """A class to perform benchmarking on various stringart solving methods.

        Parameters
        ----------
        image : np.ndarray
            The input image to be processed.
        crop_mode : CropMode
            The mode in which the image is being processed. This determines cropping behaviour.
        number_of_pegs : int, optional
            The number of pegs used in the solving process. Default is 100.

        Attributes
        ----------
        image : np.ndarray
            The input image to be processed.
        crop_mode : CropMode
            The mode in which the image is being processed. This determines cropping behaviour.
        number_of_pegs : int, optional
            The number of pegs used in the solving process. Default is 100.
        rasterization: Rasterization, optional
            If "xiaolin-wu", the line is generated using a rasterized algorithm (Xiaolin Wu's algorithm).
            If "bresenham", the line is generated using a non-rasterized algorithm (Bresenham's algorithm).
        solver : Solver
            The solver instance configured with the provided parameters.
        benchmarks_to_run : list
            A list of tuples defining the solver methods to benchmark and their respective parameters.
        """
        crop_mode: CropMode = crop_mode if crop_mode else "center"
        number_of_pegs = number_of_pegs if number_of_pegs else 100

        self.image = image
        self.crop_mode = crop_mode
        self.number_of_pegs = number_of_pegs
        self.rasterization = rasterization

        self.solver = Solver(image, crop_mode, number_of_pegs=number_of_pegs, rasterization=rasterization)
        self.benchmarks_to_run = [  # TODO: add linear-least-squares, binary-projection-ls
            # fmt: off
            (self.solver.least_squares, {"matrix_representation": "dense"}),
            (self.solver.least_squares, {"matrix_representation": "sparse"}),
            (self.solver.matching_pursuit, {"number_of_lines": 1000, "mp_method": "orthogonal"}),
            (self.solver.matching_pursuit, {"number_of_lines": 1000, "mp_method": "greedy", "selector_type": "random"}),
            (self.solver.matching_pursuit, {"number_of_lines": 1000, "mp_method": "greedy", "selector_type": "dot-product"}),
            # fmt: on
        ]

    def run_benchmark(self, func: Callable, *args, **kwargs) -> BenchmarkResult:
        """Benchmark a function by measuring its execution time and peak memory usage.

        Parameters
        ----------
        func : Callable
           The function to be benchmarked.
        *args : tuple
           Positional arguments passed to the function.
        **kwargs : dict
           Keyword arguments passed to the function.

        Returns
        -------
        BenchmarkResult
           A BenchmarkResult instance containing the output of the function,
           elapsed time, and peak memory usage.
        """

        time_start = time.monotonic()
        tracemalloc.start()

        A, x = func(*args, **kwargs)
        output = self.solver.compute_solution(A, x)

        time_end = time.monotonic()
        elapsed_monotonic_time = time_end - time_start

        _, peak_memory_usage = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        elapsed_time: ElapsedTime = convert_monotonic_time(elapsed_monotonic_time)
        peak_memory_size: MemorySize = convert_memory_size(peak_memory_usage)

        return BenchmarkResult(
            solver=func.__name__,
            params=kwargs,
            number_of_pegs=self.number_of_pegs,
            output_image=output,
            output_image_path=None,
            elapsed_monotonic_time=elapsed_monotonic_time,
            peak_memory_usage=peak_memory_usage,
            elapsed_time=elapsed_time,
            peak_memory_size=peak_memory_size,
        )

    def run_benchmarks(self) -> List[BenchmarkResult]:
        """Run a series of benchmark tests on a given image using different solvers and configurations.

        Returns
        -------
        List[BenchmarkResult]
            A list of benchmark results, each containing the solver, parameters, and performance
            metrics (e.g., elapsed time, memory usage) for each benchmark run.

        Notes
        -----
        This function runs the following benchmarks:
        1. **Least Squares Solver** with a dense matrix method.
        2. **Least Squares Solver** with a sparse matrix method.
        3. **Matching Pursuit Solver** with orthogonal method.
        4. **Matching Pursuit Solver** with greedy solver, 1000 lines and a random selector type.
        5. **Matching Pursuit Solver** with greedy solver, 1000 lines and a dot-product selector type.
        """

        results: List[BenchmarkResult] = []

        for benchmark_to_run in tqdm(self.benchmarks_to_run, desc="Running Benchmarks"):
            func, params = benchmark_to_run

            logger.info(f"===== Starting Benchmark =====")
            logger.info(f"Function: {func.__name__}")
            logger.info(f"Parameters: {params}")

            try:
                result = self.run_benchmark(func, **params)
                results.append(result)
            except Exception as e:
                logger.exception(f"Benchmark failed for {func.__name__} with params {params}. Exception: {e}")
                continue

            logger.info(f"Completed Function: {func.__name__}")

        return results

    @staticmethod
    def save_benchmarks(benchmark_results: List[BenchmarkResult], filename: str = "benchmark") -> None:
        """Save benchmark results to a JSON file.

        Parameters
        ----------
            benchmark_results: List[BenchmarkResult]
                List of benchmark results to save.
            filename: str
                Path to the output JSON file.
        """

        directory = Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH / filename
        os.makedirs(directory, exist_ok=True)

        # save each output_image to a directory inside benchmarks named after filename
        for index in range(len(benchmark_results)):
            output_image = benchmark_results[index].output_image

            image_name = f"image_{index:03}.png"
            io.imsave(directory / image_name, output_image)

            benchmark_results[index].output_image_path = str(directory / image_name)

        json_results = [result.to_json() for result in benchmark_results]
        filepath = Benchmark.BENCHMARKS_PATH / f"{filename}.json"
        with open(filepath, "w") as file:
            json.dump(json_results, file, indent=4)

        logger.info(f"Benchmarks saved to: {filepath}")

    @staticmethod
    def load_benchmarks(filename: str) -> List[BenchmarkResult]:
        """Load benchmark data from a JSON file, process the data, and return a list of
        BenchmarkResult objects.

        Parameters
        ----------
        filename : str
            The name of the JSON file (without the .json extension) that contains the benchmark data.
            The benchmark data should be found in the `benchmarks` directory.

        Returns
        -------
        List[BenchmarkResult]
            A list of `BenchmarkResult` objects created from the benchmark data in the JSON file.
        """
        filepath = Benchmark.BENCHMARKS_PATH / f"{filename}.json"
        with open(filepath, "r") as file:
            benchmarks = json.load(file)

        for benchmark in benchmarks:
            benchmark["output_image"] = io.imread(benchmark["output_image_path"])
            benchmark.pop("output_image_shape")

        benchmarks_class = [
            BenchmarkResult(**benchmark) for benchmark in benchmarks  # unpack dict values into constructor
        ]

        logger.info(f"Loaded benchmarks: {filepath}")
        return benchmarks_class

    def run_analysis(
        self, benchmarks: List[BenchmarkResult], ground_truth_image: np.ndarray, dirname: str = "analysis"
    ) -> None:
        """Perform an analysis of benchmarking results by generating and saving plots that show the differences
        between output images and the ground truth image, as well as RMSE and time and memory usage for each benchmark.

        Parameters
        ----------
        benchmarks : List[BenchmarkResult]
            A list of benchmark results to analyze.

        ground_truth_image : np.ndarray
            The ground truth image to compare the benchmarked images against. It is expected to be a
            2D array representing the image in a NumPy array format.

        dirname : str, optional, default: "analysis"
            The directory where the analysis results (plots) will be saved. If the directory does not
            exist, it will be created inside the `docs/plots` directory.
        """

        directory = self.PLOTS_PATH / dirname
        os.makedirs(directory, exist_ok=True)

        # images are scaled in [0, 1] range because the solvers return them in range [0, 255]
        output_images = [ImageWrapper.scale_image(benchmark.output_image) for benchmark in benchmarks]
        ground_truth_image = crop_image(ground_truth_image, self.crop_mode)

        labels = []
        for benchmark in benchmarks:
            params_str = "\n".join(f"{key}: {value}" for key, value in benchmark.params.items())
            label = f"{benchmark.solver}\n{params_str}"
            labels.append(label)

        # plot diff images and rmses
        rmses = [normalized_root_mse(ground_truth_image, test_image) for test_image in output_images]
        diff_images = [
            ImageWrapper.scale_image(np.abs(ground_truth_image - test_image)) for test_image in output_images
        ]
        diff_images = prepare_diff_images(diff_images, self.crop_mode)

        fig, axs = plt.subplots(1, len(output_images), figsize=(16, 6))
        plot_name = "Difference Images"

        # plt.subplots behaves differently when the length of output_images is 1
        if len(output_images) == 1:
            axs = [axs]

        for index, diff_image in enumerate(diff_images):
            axs[index].imshow(diff_image, cmap="plasma")
            axs[index].axis("off")
            axs[index].text(
                0.5,
                -0.2,
                f"{labels[index]}\nRMS: {rmses[index]:.4f}",
                ha="center",
                va="center",
                transform=axs[index].transAxes,
                fontsize=10,
            )
        fig.tight_layout()
        fig.savefig(f"{directory / plot_name}.png", format="png")
        fig.show()

        # plot time and memory
        monotonic_time = [benchmark.elapsed_monotonic_time for benchmark in benchmarks]
        memory_size = [benchmark.peak_memory_usage / (1024**2) for benchmark in benchmarks]

        def plot_bar_graph(name: str, ylabel: str, values: List | np.ndarray, color: str = "skyblue") -> None:
            plt.figure(figsize=(10, 6))

            plt.bar(labels, values, color=color)

            plt.xlabel("Solvers")
            plt.ylabel(ylabel)

            plt.xticks(rotation=45)
            plt.tight_layout()

            plt.savefig(f"{directory / name}.png", format="png")
            plt.show()

        # time plot
        plot_bar_graph("Time Usage", "Time (s)", monotonic_time, "skyblue")
        # memory plot
        plot_bar_graph("Memory Usage", "Memory (MB)", memory_size, "orange")

        logger.info(f"Analysis saved to: {directory}")


def prepare_diff_images(
    diff_images: list[np.ndarray], crop_mode: CropMode, colormap: str | Colormap = "plasma"
) -> list[np.ndarray]:
    """Processes a list of difference images by applying an alpha map and colormap to each image, converting them to RGBA.

    Parameters
    ----------
    diff_images : list of np.ndarray
        A list of 2D NumPy arrays (height, width) representing difference images. Each image should be in black-and-white (grayscale).
    crop_mode : CropMode
        The cropping mode that was used on the diff_images.
    colormap : str or matplotlib.colors.Colormap, optional
        The name of the Matplotlib colormap to apply or a Colormap object. Default is 'plasma'.

    Returns
    -------
    list of np.ndarray
        A list of 3D NumPy arrays (height, width, 4) representing the processed RGBA images. Each image will have the
        red, green, and blue channels updated with a colormap (plasma), and the alpha channel applied from the corresponding
        alpha map based on the mode.
    """
    cmap = plt.get_cmap(colormap) if isinstance(colormap, str) else colormap

    def apply_cmap_bw_to_rgb(rgba_image: np.ndarray) -> np.ndarray:
        bw_image = rgba_image[..., 0]

        colorized = cmap(bw_image)
        rgba_image[..., 0:3] = colorized[..., 0:3]

        return rgba_image

    diff_images = [
        apply_cmap_bw_to_rgb(
            ImageWrapper.apply_alpha_map_bw_to_rgba(diff_image, ImageWrapper.alpha_map(diff_image, crop_mode))
        )
        for diff_image in diff_images
    ]

    return diff_images
