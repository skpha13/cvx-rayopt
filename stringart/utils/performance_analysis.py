import json
import time
import tracemalloc
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, List

import numpy as np
from stringart.solver import Solver
from stringart.utils.image import ImageWrapper
from stringart.utils.time_and_memory_utils import (
    ElapsedTime,
    MemorySize,
    convert_memory_size,
    convert_monotonic_time,
    format_memory_size,
    format_time,
)
from stringart.utils.types import Mode


@dataclass
class BenchmarkResult:
    """A dataclass that stores the results of a benchmark, including the output image,
    elapsed time, and peak memory usage.

    Attributes
    ----------
    solver: str
        The name of the solver ran by the benchmark.
    params: dict
        The params passed to the solver.
    output_image : np.ndarray
        The image output generated by the benchmarked function.
    elapsed_monotonic_time : float
        The elapsed time in seconds measured with `time.monotonic()`.
    peak_memory_usage : int
        The peak memory usage in bytes measured by `tracemalloc.get_traced_memory()`.
    elapsed_time : ElapsedTime
        The elapsed time as a dictionary containing hours, minutes, seconds, and milliseconds.
    peak_memory_size : MemorySize
        The peak memory usage as a dictionary containing gigabytes, megabytes, kilobytes, and bytes.
    """

    solver: str
    params: dict
    output_image: np.ndarray
    elapsed_monotonic_time: float
    peak_memory_usage: int

    elapsed_time: ElapsedTime
    peak_memory_size: MemorySize

    def __str__(self):
        formatted_time = format_time(self.elapsed_time)
        formatted_memory = format_memory_size(self.peak_memory_size)

        return (
            f"Benchmark Results:\n"
            f"- Solver: {self.solver}\n"
            f"- Params: {json.dumps(self.params, indent=4)}\n"
            f"- Elapsed Time: {formatted_time}\n"
            f"- Peak Memory Usage: {formatted_memory}\n"
            f"- Output Image Shape: {self.output_image.shape}"
        )

    def to_json(self) -> dict:
        return {
            "solver": self.solver,
            "params": self.params,
            "output_image_shape": self.output_image.shape,
            "elapsed_monotonic_time": self.elapsed_monotonic_time,
            "peak_memory_usage": self.peak_memory_usage,
            "elapsed_time": self.elapsed_time,
            "peak_memory_size": self.peak_memory_size,
        }


def benchmark(func: Callable, *args, **kwargs) -> BenchmarkResult:
    """Benchmark a function by measuring its execution time and peak memory usage.

    Parameters
    ----------
    func : Callable
       The function to be benchmarked.
    *args : tuple
       Positional arguments passed to the function.
    **kwargs : dict
       Keyword arguments passed to the function.

    Returns
    -------
    BenchmarkResult
       A BenchmarkResult instance containing the output of the function,
       elapsed time, and peak memory usage.
    """

    time_start = time.monotonic()
    tracemalloc.start()

    output = func(*args, **kwargs)

    time_end = time.monotonic()
    elapsed_monotonic_time = time_end - time_start

    _, peak_memory_usage = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    elapsed_time: ElapsedTime = convert_monotonic_time(elapsed_monotonic_time)
    peak_memory_size: MemorySize = convert_memory_size(peak_memory_usage)

    return BenchmarkResult(
        solver=func.__name__,
        params=kwargs,
        output_image=output,
        elapsed_monotonic_time=elapsed_monotonic_time,
        peak_memory_usage=peak_memory_usage,
        elapsed_time=elapsed_time,
        peak_memory_size=peak_memory_size,
    )


def run_benchmarks(image: ImageWrapper) -> List[BenchmarkResult]:
    """Run a series of benchmark tests on a given image using different solvers and configurations.

    Parameters
    ----------
    image : ImageWrapper
        The input image to be processed by the solvers. This object should contain the image data
        and any necessary methods for manipulation or analysis.

    Returns
    -------
    List[BenchmarkResult]
        A list of benchmark results, each containing the solver, parameters, and performance
        metrics (e.g., elapsed time, memory usage) for each benchmark run.

    Notes
    -----
    This function runs the following benchmarks:
    1. **Least Squares Solver** with a dense matrix method.
    2. **Least Squares Solver** with a sparse matrix method.
    3. **Greedy Solver** with 1000 lines and a random selector type.
    4. **Greedy Solver** with 1000 lines and a dot-product selector type.
    """

    mode: Mode = "center"
    solver = Solver(image, mode, number_of_pegs=100)
    benchmarks_to_run = [
        (solver.least_squares, {"method": "dense"}),
        (solver.least_squares, {"method": "sparse"}),
        (solver.greedy, {"number_of_lines": 1000, "selector_type": "random"}),
        (solver.greedy, {"number_of_lines": 1000, "selector_type": "dot-product"}),
    ]
    results: List[BenchmarkResult] = []

    for benchmark_to_run in benchmarks_to_run:
        func, params = benchmark_to_run
        result = benchmark(func, **params)
        results.append(result)

    return results


def save_benchmarks(benchmark_results: List[BenchmarkResult], file_path: str | Path) -> None:
    """Save benchmark results to a JSON file.

    Parameters:
        benchmark_results (List[BenchmarkResult]): List of benchmark results to save.
        file_path (str | Path): Path to the output JSON file.
    """

    json_results = [result.to_json() for result in benchmark_results]

    with open(file_path, "w") as file:
        json.dump(json_results, file, indent=4)
