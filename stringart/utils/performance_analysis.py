import json
import logging
import os
import time
import tracemalloc
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, List

import matplotlib.pyplot as plt
import numpy as np
from skimage import io
from skimage.metrics import normalized_root_mse
from stringart.solver import Solver
from stringart.utils.image import crop_image
from stringart.utils.time_and_memory_utils import (
    ElapsedTime,
    MemorySize,
    convert_memory_size,
    convert_monotonic_time,
    format_memory_size,
    format_time,
)
from stringart.utils.types import Mode

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """A dataclass that stores the results of a benchmark, including the output image,
    elapsed time, and peak memory usage.

    Attributes
    ----------
    solver: str
        The name of the solver ran by the benchmark.
    params: dict
        The params passed to the solver.
    number_of_pegs: int, optional
        The number of pegs to be used in the string art computation. Default is 100.
    output_image : np.ndarray
        The image output generated by the benchmarked function.
    elapsed_monotonic_time : float
        The elapsed time in seconds measured with `time.monotonic()`.
    peak_memory_usage : int
        The peak memory usage in bytes measured by `tracemalloc.get_traced_memory()`.
    elapsed_time : ElapsedTime
        The elapsed time as a dictionary containing hours, minutes, seconds, and milliseconds.
    peak_memory_size : MemorySize
        The peak memory usage as a dictionary containing gigabytes, megabytes, kilobytes, and bytes.
    """

    solver: str
    params: dict
    number_of_pegs: int
    output_image: np.ndarray
    output_image_path: str | None
    elapsed_monotonic_time: float
    peak_memory_usage: int

    elapsed_time: ElapsedTime
    peak_memory_size: MemorySize

    def __str__(self):
        formatted_time = format_time(self.elapsed_time)
        formatted_memory = format_memory_size(self.peak_memory_size)

        return (
            f"Benchmark Results:\n"
            f"- Solver: {self.solver}\n"
            f"- Params: {json.dumps(self.params, indent=4)}\n"
            f"- Number of Pegs: {self.number_of_pegs}\n"
            f"- Elapsed Time: {formatted_time}\n"
            f"- Peak Memory Usage: {formatted_memory}\n"
            f"- Output Image Shape: {self.output_image.shape}\n"
            f"- Output Image Path: {self.output_image_path}"
        )

    def to_json(self) -> dict:
        return {
            "solver": self.solver,
            "params": self.params,
            "number_of_pegs": self.number_of_pegs,
            "output_image_shape": self.output_image.shape,
            "output_image_path": self.output_image_path,
            "elapsed_monotonic_time": self.elapsed_monotonic_time,
            "peak_memory_usage": self.peak_memory_usage,
            "elapsed_time": self.elapsed_time,
            "peak_memory_size": self.peak_memory_size,
        }


class Benchmark:
    PLOTS_PATH: Path | None = None
    BENCHMARKS_PATH: Path | None = None
    BENCHMARKS_IMAGE_OUTPUT_PATH: Path | None = None

    @staticmethod
    def initialize_metadata(path: Path) -> None:
        Benchmark.PLOTS_PATH = path / "docs/plots"
        Benchmark.BENCHMARKS_PATH = path / "benchmarks"
        Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH = path / "benchmarks/img_outputs"

        os.makedirs(Benchmark.PLOTS_PATH, exist_ok=True)
        os.makedirs(Benchmark.BENCHMARKS_PATH, exist_ok=True)
        os.makedirs(Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH, exist_ok=True)

    def __init__(self, image: np.ndarray, mode: Mode, number_of_pegs: int = 100):
        """A class to perform benchmarking on various stringart solving methods.

        Parameters
        ----------
        image : np.ndarray
            The input image to be processed.
        mode : Mode
            The mode in which the image is being processed. This determines cropping behaviour.
        number_of_pegs : int, optional
            The number of pegs used in the solving process. Default is 100.

        Attributes
        ----------
        image : np.ndarray
            The input image to be processed.
        mode : Mode
            The mode in which the image is being processed. This determines cropping behaviour.
        number_of_pegs : int, optional
            The number of pegs used in the solving process. Default is 100.
        solver : Solver
            The solver instance configured with the provided parameters.
        benchmarks_to_run : list
            A list of tuples defining the solver methods to benchmark and their respective parameters.
        """

        self.image = image
        self.mode = mode
        self.number_of_pegs = number_of_pegs

        self.solver = Solver(image, mode, number_of_pegs=100)
        self.benchmarks_to_run = [
            (self.solver.least_squares, {"method": "dense"}),
            (self.solver.least_squares, {"method": "sparse"}),
            (self.solver.greedy, {"number_of_lines": 1000, "selector_type": "random"}),
            (self.solver.greedy, {"number_of_lines": 1000, "selector_type": "dot-product"}),
        ]

    def run_benchmark(self, func: Callable, *args, **kwargs) -> BenchmarkResult:
        """Benchmark a function by measuring its execution time and peak memory usage.

        Parameters
        ----------
        func : Callable
           The function to be benchmarked.
        *args : tuple
           Positional arguments passed to the function.
        **kwargs : dict
           Keyword arguments passed to the function.

        Returns
        -------
        BenchmarkResult
           A BenchmarkResult instance containing the output of the function,
           elapsed time, and peak memory usage.
        """

        time_start = time.monotonic()
        tracemalloc.start()

        output = func(*args, **kwargs)

        time_end = time.monotonic()
        elapsed_monotonic_time = time_end - time_start

        _, peak_memory_usage = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        elapsed_time: ElapsedTime = convert_monotonic_time(elapsed_monotonic_time)
        peak_memory_size: MemorySize = convert_memory_size(peak_memory_usage)

        return BenchmarkResult(
            solver=func.__name__,
            params=kwargs,
            number_of_pegs=self.number_of_pegs,
            output_image=output,
            output_image_path=None,
            elapsed_monotonic_time=elapsed_monotonic_time,
            peak_memory_usage=peak_memory_usage,
            elapsed_time=elapsed_time,
            peak_memory_size=peak_memory_size,
        )

    def run_benchmarks(self) -> List[BenchmarkResult]:
        """Run a series of benchmark tests on a given image using different solvers and configurations.

        Returns
        -------
        List[BenchmarkResult]
            A list of benchmark results, each containing the solver, parameters, and performance
            metrics (e.g., elapsed time, memory usage) for each benchmark run.

        Notes
        -----
        This function runs the following benchmarks:
        1. **Least Squares Solver** with a dense matrix method.
        2. **Least Squares Solver** with a sparse matrix method.
        3. **Greedy Solver** with 1000 lines and a random selector type.
        4. **Greedy Solver** with 1000 lines and a dot-product selector type.
        """

        results: List[BenchmarkResult] = []

        for benchmark_to_run in self.benchmarks_to_run:
            func, params = benchmark_to_run
            logger.info(f"Started Function: {func.__name__}, Params: {params}\n")

            result = self.run_benchmark(func, **params)
            results.append(result)

            logger.info(f"Finished\n")

        return results

    @staticmethod
    def save_benchmarks(benchmark_results: List[BenchmarkResult], filename: str = "benchmark") -> None:
        """Save benchmark results to a JSON file.

        Parameters
        ----------
            benchmark_results: List[BenchmarkResult]
                List of benchmark results to save.
            filename: str
                Path to the output JSON file.
        """

        directory = Benchmark.BENCHMARKS_IMAGE_OUTPUT_PATH / filename
        os.makedirs(directory, exist_ok=True)

        # save each output_image to a directory inside benchmarks named after filename
        for index in range(len(benchmark_results)):
            output_image = benchmark_results[index].output_image

            image_name = f"image_{index:03}.png"
            io.imsave(directory / image_name, output_image)

            benchmark_results[index].output_image_path = str(directory / image_name)

        json_results = [result.to_json() for result in benchmark_results]
        with open(Benchmark.BENCHMARKS_PATH / f"{filename}.json", "w") as file:
            json.dump(json_results, file, indent=4)

    @staticmethod
    def load_benchmarks(filename: str) -> List[BenchmarkResult]:
        """Load benchmark data from a JSON file, process the data, and return a list of
        BenchmarkResult objects.

        Parameters
        ----------
        filename : str
            The name of the JSON file (without the .json extension) that contains the benchmark data.
            The benchmark data should be found in the `benchmarks` directory.

        Returns
        -------
        List[BenchmarkResult]
            A list of `BenchmarkResult` objects created from the benchmark data in the JSON file.
        """
        benchmarks: List = []

        with open(Benchmark.BENCHMARKS_PATH / f"{filename}.json", "r") as file:
            benchmarks = json.load(file)

        for benchmark in benchmarks:
            benchmark["output_image"] = io.imread(benchmark["output_image_path"])
            benchmark.pop("output_image_shape")

        benchmarks_class = [
            BenchmarkResult(**benchmark) for benchmark in benchmarks  # unpack dict values into constructor
        ]

        return benchmarks_class

    def run_analysis(
        self, benchmarks: List[BenchmarkResult], ground_truth_image: np.ndarray, dirname: str = "analysis"
    ) -> None:
        """Perform an analysis of benchmarking results by generating and saving plots that show the differences
        between output images and the ground truth image, as well as RMSE and time and memory usage for each benchmark.

        Parameters
        ----------
        benchmarks : List[BenchmarkResult]
            A list of benchmark results to analyze.

        ground_truth_image : np.ndarray
            The ground truth image to compare the benchmarked images against. It is expected to be a
            2D array representing the image in a NumPy array format.

        dirname : str, optional, default: "analysis"
            The directory where the analysis results (plots) will be saved. If the directory does not
            exist, it will be created inside the `docs/plots` directory.
        """

        directory = self.PLOTS_PATH / dirname
        os.makedirs(directory, exist_ok=True)

        output_images = [benchmark.output_image for benchmark in benchmarks]
        ground_truth_image = crop_image(ground_truth_image, self.mode)

        labels = [
            f"{benchmark.solver}\n{"\n".join(f"{key}: {value}" for key, value in benchmark.params.items())}"
            for benchmark in benchmarks
        ]

        # plot diff images and rmses
        rmses = [normalized_root_mse(ground_truth_image, test_image) for test_image in output_images]
        diff_images = [ground_truth_image - test_image for test_image in output_images]

        fig, axs = plt.subplots(1, len(output_images), figsize=(16, 6))
        plot_name = "Difference Images"
        fig.suptitle(plot_name)

        for index, diff_image in enumerate(diff_images):
            axs[index].imshow(diff_image, cmap="plasma")
            axs[index].axis("off")
            axs[index].text(
                0.5,
                -0.1,
                f"{labels[index]}\nRMS: {rmses[index]:.4f}",
                ha="center",
                va="center",
                transform=axs[index].transAxes,
                fontsize=10,
            )
        fig.tight_layout()
        fig.savefig(f"{directory / plot_name}.svg", format="svg")
        fig.show()

        # plot time and memory
        monotonic_time = [benchmark.elapsed_monotonic_time for benchmark in benchmarks]
        memory_size = [benchmark.peak_memory_usage / (1024**2) for benchmark in benchmarks]

        def plot_bar_graph(name: str, ylabel: str, values: List | np.ndarray, color: str = "skyblue") -> None:
            plt.figure(figsize=(10, 6))

            plt.bar(labels, values, color=color)

            plt.xlabel("Solvers")
            plt.ylabel(ylabel)

            plt.title(name)
            plt.xticks(rotation=45)
            plt.tight_layout()

            plt.savefig(f"{directory / name}.svg", format="svg")
            plt.show()

        # time plot
        plot_bar_graph("Time Usage", "Time (s)", monotonic_time, "skyblue")
        # memory plot
        plot_bar_graph("Memory Usage", "Memory (MB)", memory_size, "orange")
